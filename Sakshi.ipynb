{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = [\n",
    "    \"amaravati\",\n",
    "    \"anantapur\",\n",
    "    \"shri sathya sai\",\n",
    "    \"chittoor\",\n",
    "    \"tirupati\",\n",
    "    \"east godavari\",\n",
    "    \"kakinada\",\n",
    "    \"dr. br ambedkar konaseema\",\n",
    "    \"guntur\",\n",
    "    \"bapatla\",\n",
    "#     \"badass\",\n",
    "    \"palnadu\",\n",
    "    \"krishna\",\n",
    "    \"ntr\",\n",
    "    \"kurnool\",\n",
    "    \"nandyala\",\n",
    "    \"brightness\",\n",
    "    \"sripotti sriramulu nellore\",\n",
    "    \"srikakulam\",\n",
    "    \"anakapalli\",\n",
    "    \"visakhapatnam\",\n",
    "    \"alluri sitaramaraj\",\n",
    "    \"vizianagaram\",\n",
    "    \"parvathipuram manyam\",\n",
    "    \"west godavari\",\n",
    "    \"elur\",\n",
    "    \"ysr\",\n",
    "#     \"brother\",\n",
    "    \"annamayya\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c677349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time\n",
    "import Sakshi_Main as sa\n",
    "import eenadu_Main as ee\n",
    "\n",
    "def process_district_news(news_source, district):\n",
    "    max_retries = 3\n",
    "    for _ in range(max_retries):\n",
    "        try:\n",
    "            news_source.TodaysNews(district)\n",
    "            print(f\"Processing news from {news_source.__name__} for district {district} completed.\")\n",
    "            time.sleep(2)\n",
    "            break  \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing news from {news_source.__name__} for district {district}: {e}\")\n",
    "            time.sleep(2)  \n",
    "    else:\n",
    "        print(f\"Failed to process news from {news_source.__name__} for district {district}\")\n",
    "\n",
    "def process_news_for_source(news_source, districts):\n",
    "    threads = []\n",
    "    for district in districts:\n",
    "        thread = threading.Thread(target=process_district_news, args=(news_source, district))\n",
    "        threads.append(thread)\n",
    "\n",
    "    \n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "   \n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "\n",
    "\n",
    "eenadu_thread = threading.Thread(target=process_news_for_source, args=(ee, districts))\n",
    "eenadu_thread.start()\n",
    "eenadu_thread.join()\n",
    "\n",
    "print(\"All news processing is complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908aa0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news = pd.read_csv('Data/Sakshi/amaravati/Sakshi_2023-08-18_data.csv')\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1cfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trans_list = list(df_news.News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trans_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53252749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-3.3B\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-3.3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf68ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trans_text = []\n",
    "\n",
    "for n in Trans_list:\n",
    "    article = f\"translate Te: {n} to En:\"\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated_tokens = model.generate(**inputs)\n",
    "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    Trans_text.append(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015a5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trans_text = []\n",
    "for n in Trans_list:\n",
    "    article = n\n",
    "    inputs = tokenizer(article, return_tensors=\"pt\", source_lang=\"te\", target_lang=\"en\")\n",
    "    translated_tokens = model.generate(**inputs)\n",
    "    Trans_text.append(tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde7953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trans_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b54459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import sys\n",
    "#import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "import nltk\n",
    "#import pycountry\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14de3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
    "for index, row in tw_list['text'].items():\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    if neg > pos:\n",
    "        tw_list.loc[index, 'sentiment'] = 'negative'\n",
    "    elif pos > neg:\n",
    "        tw_list.loc[index, 'sentiment'] = 'positive'\n",
    "    else:\n",
    "        tw_list.loc[index, 'sentiment'] = 'neutral'\n",
    "        tw_list.loc[index, 'neg'] = neg\n",
    "        tw_list.loc[index, 'neu'] = neu\n",
    "        tw_list.loc[index, 'pos'] = pos\n",
    "        tw_list.loc[index, 'compound'] = comp\n",
    "tw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246958d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_list_negative = tw_list[tw_list['sentiment']=='negative']\n",
    "tw_list_positive = tw_list[tw_list['sentiment']=='positive']\n",
    "tw_list_neutral = tw_list[tw_list['sentiment']=='neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edbde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_values_in_column(data,feature):\n",
    "    total=data.loc[:,feature].value_counts(dropna=False)\n",
    "    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
    "    return pd.concat([total,percentage],axis=1,keys=['Total','Percentage'])\n",
    "#Count_values for sentiment\n",
    "sentiment_df = count_values_in_column(tw_list,'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb868034",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d220cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.pie(sentiment_df,values=sentiment_df.Percentage,names=sentiment_df.index)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d712c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating tweet’s lenght and word count\n",
    "tw_list[\"text_len\"] = tw_list['text'].astype(str).apply(len)\n",
    "tw_list[\"text_word_count\"] = tw_list['text'].apply(lambda x: len(str(x).split()))\n",
    "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b87e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.DataFrame(tw_list.groupby('sentiment').text_word_count.mean()),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0294e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Punctuation\n",
    "def remove_punct(text):\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0–9]+', '', text)\n",
    "    return text\n",
    "tw_list['punct'] = tw_list['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "#Appliyng tokenization\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "#Removing stopwords\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "#Appliyng Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))\n",
    "\n",
    "#Cleaning Text\n",
    "def clean_text(text):\n",
    "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
    "    return text\n",
    "tw_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62554c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appliyng Countvectorizer\n",
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(tw_list['text'])\n",
    "print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2458bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names_out())\n",
    "count_vect_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ac8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = pd.DataFrame(count_vect_df.sum())\n",
    "countdf = count.sort_values(0,ascending=False).head(20)\n",
    "countdf[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to ngram\n",
    "def get_top_n_gram(corpus,ngram_range,n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "#n2_bigram\n",
    "n2_bigrams = get_top_n_gram(tw_list['text'],(2,2),10)\n",
    "n2_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb497488",
   "metadata": {},
   "outputs": [],
   "source": [
    "n3_trigrams = get_top_n_gram(tw_list['text'],(3,3),20)\n",
    "n3_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81ed25",
   "metadata": {},
   "outputs": [],
   "source": [
    "n4_trigrams = get_top_n_gram(tw_list['text'],(4,4),10)\n",
    "n4_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dda934",
   "metadata": {},
   "outputs": [],
   "source": [
    "n5_trigrams = get_top_n_gram(tw_list['text'],(5,5),10)\n",
    "n5_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399624bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n6_trigrams = get_top_n_gram(tw_list['text'],(6,6),10)\n",
    "n6_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bc3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [i[0] for i in n3_trigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8dd9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# iterate through the csv file\n",
    "for val in word_list:\n",
    "    val = str(val)\n",
    "    tokens = val.split()\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    "\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "background_color ='white',\n",
    "stopwords = stopwords,\n",
    "min_font_size = 10).generate(comment_words)\n",
    "\n",
    "# plot the WordCloud image\t\t\t\t\t\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03920e",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16120c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10 = pd.DataFrame()\n",
    "top_10['text'] = df1.Trans_text[:10]\n",
    "top_10 = top_10.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe2782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_rt = lambda x:re.sub('RT @\\w+: ',\" \",x)\n",
    "rt = lambda x: re.sub(re.escape(\"(@[A-Za-z0–9]+)|([⁰-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\"),\" \",x)\n",
    "top_10[\"text\"] = top_10.text.map(remove_rt).map(rt)\n",
    "top_10[\"text\"] = top_10.text.str.lower()\n",
    "top_10.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = ''\n",
    "for i in list(top_10.text):\n",
    "    Text+=i\n",
    "Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36040bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "result = summarizer(Text, max_length=200, min_length=100, do_sample=False)\n",
    "result = result[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f798f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
